% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

\usepackage{scrextend}
\usepackage{blindtext}
\addtokomafont{labelinglabel}{\sffamily}
\usepackage{graphicx}
\usepackage{fancyvrb}
\newcommand{\verbatimfont}[1]{\renewcommand{\verbatim@font}{\ttfamily#1}}

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{Project P5: Identify Fraud from Enron Email}
\author{Peter Eisenschmidt}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section{Data Understanding and Exploration}

\subsection{Dataset Description}

total number of data points

allocation across classes (POI/non-POI)

number of features used

are there features with many missing values? etc.


\subsection{Outlier Removal}

The first step is to detect any outliers and to see if they need to be removed from the dataset. The TOTAL entry is removed, as it details the sum over all persons included in the dataset.  Furthermore, the entry THE TRAVEL AGENCY IN THE PARK is removed, as this is not a person but a corporation. \medskip

In order to detect further entries that need to be excluded, boxplots are created for all the features, as this allows detecting outliers visually.\medskip

Three parameters show outliers that require further investigation:

%{\fontfamily{ppl}\selectfont this is a test} und weiter

\begin{labeling}{restricted\_stock\_deferred}
\item [loan\_advances] Only few data points are available for this parameter. One extreme point (81,525,000) sticks out. As this data point belongs to Ken Lay, it should be considered as valid.
\item [restricted\_stock\_deferred] Only few data points available, most of them negative. The positive value requires further checking. 
\item [restricted\_stock] One negative value whereas the remaining data points are positive.
\end{labeling}

The entries where restricted\_stock\_deferred restricted\_stock show outliers belong to BHATNAGAR SANJAY and BELFER ROBERT. These entries are then removed from the dataset.\medskip

The parameters total\_payments and total\_stockvalue are the sum of all payments (salary, bonus, etc) and stock values (exercised stock option, restricted stock option, and restricted stock option deferred) respectively. So, either all other values are excluded and only the sums are included or vice versa.

\subsection{First Feature Selection}
Lastly, it can be seen that a lot of parameters contain zeros (i.e. NaNs in the original dictionary). As this may influence the performance of the algorithm, all features which contain more than 75\% of zeros are removed.\medskip

The retained parameters are:

{\fontfamily{pcr}\selectfont['poi', 'salary', 'deferral\_payments', 'bonus', 'deferred\_income', 'expenses', 'exercised\_stock\_options', 'other', 'long\_term\_incentive', 'restricted\_stock', 'to\_messages', 'from\_poi\_to\_this\_person', 'from\_messages', 'from\_this\_person\_to\_poi', 'shared\_receipt\_with\_poi']}

\subsection{Feature Creation}

The dataset contains information about the numbers of emails a person sent and received ('to\_messages' and 'from\_messages'). Furthermore, the parameters 'from\_poi\_to\_this\_person' and 'from\_this\_person\_to\_poi' indicate the numbers of emails exchanged with a POI.\medskip

The features that are created are the fractions of POI-related emails and total emails. This is done because it emphasizes the importance of POI-related emails. For example, if someone sent 8 emails to a POI and sent only 10 emails in total, this must be weighed higher against someone who sent 15 emails to a POI but has a total of 1000 emails.\medskip

The new features are \textbf{'frac\_to\_poi'} and  \textbf{'frac\_from\_poi'}.

\section{Algorithm Selection and Tuning}

\subsection{Assessment of Several Algorithms}
As a first step, 5 algorithms are selected and run with their default parameters:
\begin{enumerate}
\item Gaussian Naives Bayes
\item K Nearest Neighbors
\item Decision Tree Classifier
\item Random Forest Classifier
\item Support Vector Classifier
\end{enumerate}

The dataset is split into a training and a test dataset using {\fontfamily{pcr}\selectfont train\_test\_split}; the size of the test set is .2.\medskip

The results on the selected test set are:
\begin{verbatim}
Step 1:  0 GaussianNB
Precision:  0.75
Recall:     0.6
F1:         0.666666666667
done...
Step 1:  1 KNearestNeighbors
Precision:  0.0
Recall:     0.0
F1:         0.0
done...
Step 1:  2 Decision Tree
Precision:  1.0
Recall:     0.2
F1:         0.333333333333
done...
Step 1:  3 Random Forest
Precision:  0.0
Recall:     0.0
F1:         0.0
done...
Step 1:  4 Support Vector Classifier
Precision:  0.0
Recall:     0.0
F1:         0.0
done...
\end{verbatim}


The results evaluated with tester.py are:
{\tiny
\begin{verbatim}
GaussianNB(priors=None)
        Accuracy: 0.82571       Precision: 0.34637      Recall: 0.24800 F1: 0.28904     F2: 0.26293
        Total predictions: 14000        True positives:  496    False positives:  936   False negatives: 1504   True negatives: 11064

KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')
        Accuracy: 0.88079       Precision: 0.82515      Recall: 0.21000 F1: 0.33479     F2: 0.24680
        Total predictions: 14000        True positives:  420    False positives:   89   False negatives: 1580   True negatives: 11911

DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=42, splitter='best')
        Accuracy: 0.81121       Precision: 0.32247      Recall: 0.29200 F1: 0.30648     F2: 0.29763
        Total predictions: 14000        True positives:  584    False positives: 1227   False negatives: 1416   True negatives: 10773

RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,
            verbose=0, warm_start=False)
        Accuracy: 0.84736       Precision: 0.38487      Recall: 0.11450 F1: 0.17649     F2: 0.13322
        Total predictions: 14000        True positives:  229    False positives:  366   False negatives: 1771   True negatives: 11634

Got a divide by zero when trying out: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=42, shrinking=True,
  tol=0.001, verbose=False)
Precision or recall may be undefined due to a lack of true positive predicitons.
\end{verbatim}
}

It can be seen that most classifiers do not perform well on the test set; only the Gaussian Naives Bayes and the Decision Tree yield results. Whereas on the test set the GaussianNB has precision and recall greater than .3 (required value), using tester.py the performance is not as good, albeit close. The Decision Tree is already close using tester.py but has only a recall of .2 on the test set. Interestingly, the K Nearest Neighbor algorithm has values of 0 for recall and precision on the test set, but achieves significantly better results with tester.py.\medskip

One important thing to note is that when the algorithms are trained with their default parameters, the scoring parameter that is used to obtain the best fit is accuracy. However, in this case, accuracy is not the best metric to assess classifier performance, as there are only limited numbers of POIs in the dataset. Therefore, if all POIs were incorrectly classified as Non-POIs, the accuracy would still be XX. This will be addressed in the next step.

\subsection{First Tuning of All Algorithms}
As the performance was generally not good for the selected five algorithms, the next step is to use {\fontfamily{pcr}\selectfont GridSearchCV} to make a first tuning of the algorithms. Furthermore, MinMax scaling is applied to all algorithms except the Decision Tree and the Random Forest.\medskip

As mentioned in the previous section, accuracy is not a good metric for this data set. Precision and recall on the other hand are better metrics in this case. What this means for this project is as follows:
\begin{labeling}{Precision}
\item [Recall] Number of True Positives (TP) divided by the sum of True Positives and False Negatives (FN). A low recall indicates that the number of False Negatives is too high, i.e. a lot of employees are predicted not to be a POI whereas in reality they were
\item [Precision] Number of True Positives (TP) divided by the sum of True Positives and False Positives (FP). A low precision indicates that the number of False Positive is too high, i.e. a lot of employed are predicted to be a POI whereas in reality they were not
\end{labeling}
In both cases the number of True Positives may also be too low, i.e. the number of correctly identified POIs\medskip

It is arguably which one is more important but I would tend to achieve a better recall than precision, as it seems better to falsely identify someone as POI (and to exonerate them later) than to miss a potential POI. On the other hand, optimizing the algorithm for a good recall value may lead to a low precision, i.e. too many employees are falsely identified as POIs. Therefore, the scoring parameter that is used in {\fontfamily{pcr}\selectfont GridSearchCV} is F1. F1 is defined as
\begin{equation}
     F1 = 2\frac{precision \times recall}{precision + recall}
  \end{equation}

{\fontfamily{pcr}\selectfont GridSearchCV} automatically applies cross-validation when fitting the algorithm to the training data. In this step, {\fontfamily{pcr}\selectfont StratifiedShuffleSplit} is used with 30 splits and a test size of .2.\medskip

The results on the selected test set are:
\begin{verbatim}
Step 2:  0 GaussianNB
Best score:  0.088177008177
GaussianNB(priors=None)
Precision:  0.75
Recall:     0.6
F1:         0.666666666667
done...
 
Step 2:  1 KNearestNeighbors
Best score:  0.08
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=3, p=2,
           weights='uniform')
Precision:  0.0
Recall:     0.0
F1:         0.0
done...
 
Step 2:  2 Decision Tree
Best score:  0.333095238095
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=6,
            min_samples_split=10, min_weight_fraction_leaf=0.0,
            presort=False, random_state=42, splitter='best')
Precision:  1.0
Recall:     0.2
F1:         0.333333333333
done...
 
Step 2:  3 Random Forest
Best score:  0.176931216931
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=5, n_jobs=1, oob_score=False, random_state=42,
            verbose=0, warm_start=False)
Precision:  0.0
Recall:     0.0
F1:         0.0
done...
 
Step 2:  4 Support Vector Classifier
Best score:  0.24246031746
SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma=1, kernel='sigmoid',
  max_iter=-1, probability=False, random_state=42, shrinking=True,
  tol=0.001, verbose=False)
Precision:  0.666666666667
Recall:     0.4
F1:         0.5
done...
\end{verbatim}

The results evaluated with tester.py are:
{\tiny
\begin{verbatim}
GaussianNB(priors=None)
        Accuracy: 0.82571       Precision: 0.34637      Recall: 0.24800 F1: 0.28904     F2: 0.26293
        Total predictions: 14000        True positives:  496    False positives:  936   False negatives: 1504   True negatives: 11064

KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=3, p=2,
           weights='uniform')
        Accuracy: 0.86086       Precision: 0.53533      Recall: 0.19700 F1: 0.28801     F2: 0.22550
        Total predictions: 14000        True positives:  394    False positives:  342   False negatives: 1606   True negatives: 11658

DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=6,
            min_samples_split=10, min_weight_fraction_leaf=0.0,
            presort=False, random_state=42, splitter='best')
        Accuracy: 0.85000       Precision: 0.46318      Recall: 0.31450 F1: 0.37463     F2: 0.33608
        Total predictions: 14000        True positives:  629    False positives:  729   False negatives: 1371   True negatives: 11271

RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=5, n_jobs=1, oob_score=False, random_state=42,
            verbose=0, warm_start=False)
        Accuracy: 0.83379       Precision: 0.34903      Recall: 0.18900 F1: 0.24522     F2: 0.20808
        Total predictions: 14000        True positives:  378    False positives:  705   False negatives: 1622   True negatives: 11295

Got a divide by zero when trying out: SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma=1, kernel='sigmoid',
  max_iter=-1, probability=False, random_state=42, shrinking=True,
  tol=0.001, verbose=False)
Precision or recall may be undefined due to a lack of true positive predicitons.
\end{verbatim}
}

This shows the importance of parameter tuning. In the previous step, the entire training data is used and the classifier is fit to that. However, the performance on the test set was bad in most cases. This is most likely due to over-fitting, which then means that it does not generalize well to the new data. Parameter tuning allows controlling over-fitting so that performance improves for the new, unknown test data. \medskip

Here, with only a few parameters that are tuned, it can be seen that, especially for the Decision Tree and Random Forest, the performance with tester.py increased.

Furthermore, it shows that scaling is important for classifiers such as Support Vector Machines, as it maximizes the Euclidean distance between two clusters. Previously, there were no results on the test set but now it achieves a precision and recall of .667 and .4 respectively. The fact that it fails with tester.py shows that more tuning is necessary though.

\subsection{Algorithm Selection}
For the final algorithm the decision tree is selected, as this algoritm gives the best results with tester.py. The next step is to further tune this algorithm using {\fontfamily{pcr}\selectfont GridSearchCV}; the parameters that are tuned are:
\begin{itemize}
\item criterion
\item  max\_features
\item  max\_depth
\item  min\_samples\_split
\item  min\_samples\_leaf
\item  max\_leaf\_nodes
\item  splitter
\item  class\_weight
\item  presort
\end{itemize}
The number of splits of {\fontfamily{pcr}\selectfont StratifiedShuffleSplit} is changed to 60 (in the previous section it was 30).\medskip

The results on the selected test set are:

The results evaluated with tester.py are:

The importance of the features is checked with {\fontfamily{pcr}\selectfont feature\_importances\_} attribute of the decision tree:
\begin{verbatim}
test 
\end{verbatim}

\section{Feature Selection}

\section{Final Validation}

The previous section have highlighted the importance of validation. In this case, the available data was split into a training and a test set. In this case, the test set was determined by the random state set in {\fontfamily{pcr}\selectfont train\_test\_split} (here fixed to 42 in order to get reproducable results for this project). \medskip

With this test set and the parameter tuning it was possible to achieve good results in terms of precision and recall; both were (for my test set) greater than the required values of .3 (see previous section). However, with the tester function, the results were not as good.\medskip

This shows that it is very important 

\end{document}
